% TODO
% Say what are we solving (problem formulation, underlying application, math, discretization, etc), and discuss the different parallelization strategies along with pros and cons (maybe in subsections). First the GPU-only case and then the hybrid.

Given the continuous 4th order PDE, \cref{eq:diff4}, we discretize the two derivatives in space and time.
We choose 2nd order central-differences for the space derivatives along with an explicit Euler time-stepping scheme.
Hence, we obtain the following the stencils:

\begin{subequations}				
\label{eq:stencil}
\begin{align}
\begin{split}
&\boldsymbol\Delta \phi_{i, j}^{n}=\\
&\left(-4 \phi_{i, j}^{n}+\phi_{i-1, j}^{n}+\phi_{i+1, j}^{n}+\phi_{i, j-1}^{n}+\phi_{i, j+1}^{n}\right) / \Delta x \Delta y 
\end{split}
\\
\begin{split}
&\partial_{t} \phi_{i, j}^{n} =\left(\phi_{i, j}^{n+1}-\phi_{i, j}^{n}\right) / \Delta t
\end{split}
\end{align}
\end{subequations}

The truncation of the original continuous domain implies that suitable boundary conditions need to be imposed.
We assume periodic boundary conditions.

The main skeleton of the stencil code we implemented is given below.
\begin{algorithm}
\caption{Stencil code structure}
\begin{algorithmic}[1]
\For{all iterations}
\State UpdateHalo(in) \Comment{Periodic BCs}
\State ApplyStencil(in, out) \Comment{4th-order diffusion}
\If{not in last iteration}
\State Swap(in, out)
\EndIf
\EndFor
\end{algorithmic}
\end{algorithm}

It should be noted, that in the UpdateHalo() step we impose the boundary conditions.
Moreover, we need to mention that since we have to deal with a 4th-order diffusion term i.e. \cref{eq:diff4}, the stencil \cref{eq:stencil} is applied twice inside the ApplyStencil() routine.

\subsection{Stencils on Multicore CPUs}
\mypar{OpenMP}
In our CPU-only stencil code implementation we utilize OpenMP to achieve multithreading. 
OpenMP is a directive based programming paradigm that follows the fork-join model, where the master thread forks a specified number of slave threads and the system divides a task among them.
The threads then run concurrently.
The parallelization of a program can be achieved using simple \texttt{\#pragma omp} directives, and no substantial modification of the serial code are needed.
In our CPU-only stencil code, we parallelized the halo update using \texttt{\#pragma omp for collapse(2) nowait} and the two applications of the stencil \cref{eq:stencil} inside every z-slice using \texttt{\#pragma omp parallel for} on the 2D-loop.

\subsection{Stencils on GPUs}
\mypar{OpenMP}
OpenMP also provides support for offloading computation to different ``accelerators'', currently GPUs. This is done in a similar manner to CPU OpenMP code, by annotating code with \texttt{pragma}s (C++) or directives (Fortran). The directive \texttt{target data} can be used to manually manage data movement to and from the GPU, while the \texttt{target} directive runs the surrounded code on the accelerator. Finer tuning of parallelization can be achieved using \texttt{teams}, \texttt{distribute} and \texttt{parallel do}. Due to the hierarchical nature of these directives, loop order is just as important for achieving good performance as it is in sequential code. However, a big advantage of the OpenMP programming model, is the fact that the actual C++ or Fortran code does not require modification aside from adding directives.

\mypar{OpenACC}
OpenACC is another directive based programming paradigm for offloading work to accelerators, which is hardware agnostic, hence it can be used across different accelerators. The primary advantage of OpenACC, as with other directive based programming models, is the ease of deployment which does not require making major changes to code - such as would be needed if using a more lower level model such as CUDA. We use the \texttt{pragma acc parallel} directive to designate a scoped region where the code to be accelerated lies.  

However, to gain the most out of OpenACC, it is always good to write code in such a way that optimisations are unlocked. In our case this means using the correct loop order ( having the index with the smallest stride on the inner most loop ), eliminating dependencies and possible aliasing, and collapsing loops. The most important directive, from where the bulk of the speedup comes is \texttt{pragma acc loop}, which offloads  all the computation of the loop to the GPU.

For hybrid CPU-GPU problems, data movement is a significant bottleneck, we make use of unstructured data regions using the \texttt{pragma acc enter data } and \texttt{pragma acc exit data} to store data on the accelerator until it is manually exited. 

\mypar{CUDA}
CUDA (Compute Unified Device Architecture) is a parallel computing platform and application programming interface (API) model created by NVIDIA.
It is a software layer that gives direct access to the GPU's virtual instruction set and parallel computational elements, for the execution of compute kernels.
Writing CUDA kernels requires a lot more effort than parallelizing the code using compiler directives like in OpenMP and OpenACC.
However, it gives to the programmer more control over the application.
Additionally, since every kernel launch is asynchronous, i.e. control returns to the CPU immediately after the kernel call, it allows writing applications that utilize CPU and GPU concurrently.

For our CUDA implementation of the stencil code, we perform a 3D decomposition of the data and execute a CUDA kernel on square $8 \times 8$ blocks of every 2D slice.
In every timestep, we load the required submatrices from the device memory into the shared memory, also considering the halo update, and then we apply the two Laplacians, \cref{eq:stencil}, locally.
After the shared memory buffers are loaded, as well as after the first application of the Laplacian, local synchronization of the threads in a block is required.
This is performed by calling \texttt{\_\_synctreads()}. 
In the end of every time-loop, before proceeding to the next iteration, we add an extra barrier, i.e. \texttt{cudaDeviceSynchronize()}.
After the time-loop is completed, we load the data from the GPU memory back to RAM.
\input{src/plots/time_vs_size.tex}

\subsection{Overlapping Computations}

%\mypar{Work distribution into CPU and GPU}
Due to the nature of the physics of the atmosphere, the stencil computations at each z-level are independent of other levels, and this lends itself well to a simple decomposition of z-levels between the CPU and GPU.
Hence, during the computation of the stencil, a portion of the z-levels are offloaded to the GPU, while the rest remain on the CPU.
It is important to note that while data transfer speeds on the GPU are extremely fast, moving data to and from the CPU to the GPU is extremely expensive, and as such must be minimized. 
Therefore, once the necessary data is transferred to the GPU, it is only copied back to the CPU \textit{after} at the end of the iterations.
Since data at independent z-levels do not need information from other z-levels, even across timesteps, this leads to no dependency issues.
It should be mentioned, that at the end of every iteration we synchronize GPU and CPU.
In a distributed-memory implementation, inter-node communication would take place there.