% \input{src/plots/time_vs_size.tex}
\input{src/plots/time_vs_splitting_z_slices/256.tex}
\input{src/plots/time_vs_splitting_z_slices/512.tex}
\input{src/plots/time_vs_splitting_z_slices/1024.tex}

In this section, we examine the performance of our CPU-only, GPU-only and hybrid CPU-GPU stencil code implementations based on different programming models, i.e. OpenACC, OpenMP, and CUDA.
All the numerical experiments were performed on Piz Daint, which has the following specifications;
each hybrid computational node consists of 64GB RAM, an Intel\textsuperscript{速} Xeon\textsuperscript{速} E5-2690 v3 @ 2.60GHz (12 cores), as well as an NVIDIA\textsuperscript{速} Tesla\textsuperscript{速} P100 with 16GB.
Our CPU implementation utilizes all $12$ cores of the node using OpenMP.
Our GPU implementations are based on either directive-based programming models, i.e.\ OpenACC and OpenMP, or CUDA.
In all of our experiments we use double-precision arithmetic, and we execute the stencil application for $1024$ iterations/timesteps.

Below we give the compilers along with the respective compiler flags we used for each one of our implementations:
\begin{itemize}
\item CPU (OpenMP): GCC 8.3.0 using \texttt{-O3 -ffast-math -funroll-loops -fopenmp}
\item GPU (OpenMP): Cray Fortran using \texttt{-O3 -hfp2 -ra}
\item GPU (OpenACC): PGI C++ using  \texttt{-O3 -fast -ta=tesla,cc60}
\item GPU (CUDA):  nvcc 7.0.1 using \texttt{-arch=sm\_60}
\end{itemize}

In \cref{fig:time_vs_size}, we present the performance of our stencil code implementations on the CPU and GPU, individually, for fixed number of slices in the z-direction, i.e. $nz = 64$, and increasing sizes of the 2D slices. 


It can be observed, that the CUDA GPU-only implementation of the stencil code achieves the highest speed-up ($\sim16$x) versus the CPU-only multi-threaded implementation.
Furthermore, it can be seen that a significant speedup ($\sim$4x) can be obtained by using either of the two directive-based GPU acceleration paradigms, which involve much less effort to deploy. 

In \cref{fig:time_vs_splitting_z_slices_256,fig:time_vs_splitting_z_slices_512,fig:time_vs_splitting_z_slices_1024}, we show the performance of our CPU-only and GPU-only implementations for increasing number of z-slices on the CPU (starting from $1$) and decreasing number of z-slices on the GPU (starting from $64-1=63$), while keeping fixed the sizes of the 2D slices.
Specifically, we start with 1 slice on the CPU and 63 on the GPU, then we proceed with 2 slices on the CPU and 62 slices on the GPU, and so on.
The point where the performance lines of the CPU-only and GPU-only codes intersect, gives an estimate of the maximum number of z-slices that can be offloaded on the CPU in order to achieve maximum performance of the hybrid CPU-GPU implementations.


The \cref{fig:time_vs_splitting_z_slices_256,fig:time_vs_splitting_z_slices_512,fig:time_vs_splitting_z_slices_1024}, indicate that the CUDA implementation is already very fast, and only some minor speedup can be achieved by offloading up to 3 slices to the CPU.
Offloading more than 3 slices would lead to a performance drop since the CPU would become the bottleneck for the hybrid CPU/GPU implementation.
However, for the OpenACC and OpenMP implementations, it seems that there is more to gain by offloading part of the computation to the CPU.
The OpenACC and OpenMP implementations should be able to achieve a speed-up by offloading $\sim8$ and $\sim15$ slices, respectively.
This can simply be attributed to the fact that the codes accelerated using directives are not as optimized as manually optimised CUDA code.  

In \cref{fig:time_vs_z_slices_on_cpu_256,fig:time_vs_z_slices_on_cpu_512,fig:time_vs_z_slices_on_cpu_1024}, we present the performance of our hybrid CPU-GPU implementations for increasing number of z-slices on the CPU, and for fixed sizes of the 2D slices.  

\input{src/plots/time_vs_z_slices_on_cpu/256.tex}
\input{src/plots/time_vs_z_slices_on_cpu/512.tex}
\input{src/plots/time_vs_z_slices_on_cpu/1024.tex}

We can see that only in the case of the CUDA implementation do we actually get a speed-up by offloading some slices to the GPU.
This speed-up also agrees well with the results from \cref{fig:time_vs_z_slices_on_cpu_256,fig:time_vs_z_slices_on_cpu_512,fig:time_vs_z_slices_on_cpu_1024}, with optimal performance being reached for the case with 3 slices offloaded to the CPU.

On the other hand, for both the OpenMP and the OpenACC implementations, no performance is gained by having the CPU do part of the computation.
In both, we observe an initial jump in runtime after offloading a single slice to the CPU.
The reason for this behaviour is not immediately obvious, however, since the runtime remains approximately constant thereafter for a while, it is possible that this is the result of overheads from spawning new threads inside the OpenMP \texttt{parallel} region.
In both of these versions another jump in runtime is also observed approximately the 10th and 25th offloaded z-slice respectively. It is not completely clear what causes this jump. It is possible that at this level the size of the offloaded arrays fully populate the cache of the CPU leading to a drop in performance, however, these phenomena needs to be investigated further. 
